# Ollama/Local LLM ì§€ì› ê³„íš

> **Status**: Planning
> **Created**: 2025-01-24
> **Updated**: 2025-01-24
> **Approach**: Bì•ˆ (ê¸°ëŠ¥ë³„ í´ë°±)

## ê°œìš”

OddEyes.aiì—ì„œ Ollama ë“± ë¡œì»¬ LLM APIë¥¼ ì§€ì›í•˜ê¸° ìœ„í•œ êµ¬í˜„ ê³„íš.

### ëª©í‘œ
- OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•˜ëŠ” ë¡œì»¬ LLM ì„œë²„ ì§€ì› (Ollama, LM Studio, vLLM ë“±)
- ê¸°ì¡´ OpenAI/Anthropic ê¸°ëŠ¥ê³¼ì˜ í˜¸í™˜ì„± ìœ ì§€
- ëª¨ë¸ë³„ ì œì•½ì‚¬í•­ì— ëŒ€í•œ ìš°ì•„í•œ í´ë°±

### ë¹„ëª©í‘œ
- Ollama ì „ìš© API (`/api/generate`) ì§ì ‘ ì§€ì›
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ê´€ë¦¬ ê¸°ëŠ¥

---

## Ollama ìµœì‹  ì •ë³´ (2025ë…„ 1ì›” ê¸°ì¤€)

> ì¶œì²˜: [Ollama OpenAI Compatibility](https://ollama.com/blog/openai-compatibility), [Tool Support](https://ollama.com/blog/tool-support), [Ollama Docs](https://docs.ollama.com/capabilities/tool-calling)

### OpenAI í˜¸í™˜ API

OllamaëŠ” OpenAI Chat Completions APIì™€ í˜¸í™˜ë˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ ì œê³µ:
- **ì—”ë“œí¬ì¸íŠ¸**: `http://localhost:11434/v1`
- **API í‚¤**: `ollama` (í•„ìˆ˜ì´ì§€ë§Œ ë¯¸ì‚¬ìš©, ì•„ë¬´ ê°’ì´ë‚˜ ê°€ëŠ¥)
- **Tool Calling**: ì§€ì› (OpenAI í˜¸í™˜ í˜•ì‹)

### âš ï¸ ì¤‘ìš”: ì»¨í…ìŠ¤íŠ¸ ê¸°ë³¸ê°’

**Ollama ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ëŠ” 2048 í† í°** (ëª¨ë¸ ìµœëŒ€ê°€ ì•„ë‹˜!)

ëª¨ë¸ì´ 128kë¥¼ ì§€ì›í•´ë„ OllamaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 2048ë§Œ ì‚¬ìš©. ë³€ê²½ ë°©ë²•:

```bash
# ë°©ë²• 1: í™˜ê²½ë³€ìˆ˜
OLLAMA_CONTEXT_LENGTH=32768 ollama serve

# ë°©ë²• 2: ëŸ°íƒ€ì„ ì„¤ì •
/set parameter num_ctx 32768

# ë°©ë²• 3: Modelfileë¡œ ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±
FROM llama3.1:8b
PARAMETER num_ctx 32768
```

â†’ **ì•±ì—ì„œ `num_ctx` íŒŒë¼ë¯¸í„° ì „ë‹¬ í•„ìš”** (ë˜ëŠ” ì‚¬ìš©ìì—ê²Œ ì„¤ì • ì•ˆë‚´)

### Tool Calling ì§€ì› ëª¨ë¸

[Ollama Library](https://ollama.com/library)ì—ì„œ "tools" íƒœê·¸ í™•ì¸:

| ëª¨ë¸ | Tool Calling | ì»¨í…ìŠ¤íŠ¸ (ìµœëŒ€) | ê¶Œì¥ RAM |
|------|-------------|----------------|----------|
| **llama3.1:8b** | âœ… | 128k | 8GB+ |
| **llama3.2:3b** | âœ… | 128k | 4GB+ |
| **qwen2.5:7b** | âœ… | 32k~128k | 8GB+ |
| **qwen3:8b** | âœ… | 40k | 8GB+ |
| **mistral:7b** | âœ… | 32k | 8GB+ |
| **mixtral:8x7b** | âœ… | 32k | 48GB+ |
| gemma2:9b | âŒ | 8k | 8GB+ |
| phi-3:mini | âŒ | 128k | 4GB+ |
| llava:7b | âŒ (Visionë§Œ) | 4k | 8GB+ |

> **ê¶Œì¥**: llama3.1:8b (ê· í˜•), qwen2.5:7b (Tool Calling ìš°ìˆ˜), mistral:7b (ê°€ë²¼ì›€)

### LangChain í†µí•© ì˜µì…˜

**ì˜µì…˜ 1: ChatOpenAI + baseURL (í˜„ì¬ ê³„íš)**
```typescript
import { ChatOpenAI } from '@langchain/openai';

const llm = new ChatOpenAI({
  apiKey: 'ollama',
  model: 'llama3.1',
  configuration: {
    baseURL: 'http://localhost:11434/v1',
  },
});
```

**ì˜µì…˜ 2: @langchain/ollama íŒ¨í‚¤ì§€** (ëŒ€ì•ˆ)
```typescript
import { ChatOllama } from '@langchain/ollama';

const llm = new ChatOllama({
  model: 'llama3.1',
  baseUrl: 'http://localhost:11434',
});
```

â†’ **ChatOpenAI ë°©ì‹ ì±„íƒ** (ê¸°ì¡´ ì½”ë“œ ë³€ê²½ ìµœì†Œí™”, OpenAI/Anthropicê³¼ í†µí•© ê´€ë¦¬)

---

## ê¸°ìˆ  ë¶„ì„

### í˜„ì¬ ì½”ë“œ ì œì•½

| íŒŒì¼ | ì œì•½ | ìˆ˜ì • í•„ìš” |
|------|------|----------|
| `src/ai/client.ts` | `configuration.baseURL` ë¯¸ì „ë‹¬ | Yes |
| `src/stores/aiConfigStore.ts` | baseURL/contextLimit í•„ë“œ ì—†ìŒ | Yes |
| `src/ai/translateDocument.ts` | ì»¨í…ìŠ¤íŠ¸ í¬ê¸° í•˜ë“œì½”ë”© (200k/400k) | Yes |
| `src/ai/chat.ts` | `useResponsesApi: true` (OpenAI ì „ìš©) | Yes |
| Settings UI | ì—”ë“œí¬ì¸íŠ¸/ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì—†ìŒ | Yes |

---

## ì£¼ìš” ì´ìŠˆ ë° í•´ê²° ë°©ì•ˆ

### 1. Tool Calling

**ë¬¸ì œ**: Ollama ëª¨ë¸ ì¤‘ ì¼ë¶€ë§Œ Tool Calling ì§€ì›
- ì§€ì›: llama3.1+, qwen2.5, mistral-nemo
- ë¯¸ì§€ì›: llama3.2 (3B ì´í•˜), phi-3, gemma2

**í•´ê²°**:
```typescript
// chat.ts - Tool Calling ì‹¤íŒ¨ ì‹œ í´ë°±
try {
  return await runToolCallingLoop({ model, tools, messages });
} catch (e) {
  if (isToolCallingNotSupported(e)) {
    // í´ë°±: ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì§ì ‘ í¬í•¨í•˜ì—¬ ë‹¨ìˆœ ì±„íŒ…
    return runSimpleChatMode({
      model,
      messages: injectDocumentsToMessages(messages, sourceDoc, targetDoc),
    });
  }
  throw e;
}
```

**UX ì˜í–¥**:
- Tool Calling ë¯¸ì§€ì› ì‹œ ì±„íŒ…ì—ì„œ ë¬¸ì„œ ì¡°íšŒ ë„êµ¬ ì‚¬ìš© ë¶ˆê°€
- ë¬¸ì„œê°€ ì»¨í…ìŠ¤íŠ¸ì— ì§ì ‘ í¬í•¨ë˜ì–´ í† í° ì†Œë¹„ ì¦ê°€
- ë²ˆì—­ ê¸°ëŠ¥ì€ ì˜í–¥ ì—†ìŒ (Tool Calling ë¯¸ì‚¬ìš©)

### 2. ì»¨í…ìŠ¤íŠ¸ ì œí•œ

**ë¬¸ì œ**:
- **Ollama ê¸°ë³¸ê°’ì´ 2048 í† í°** (ëª¨ë¸ ìµœëŒ€ì™€ ë¬´ê´€!)
- ëª¨ë¸ë³„ ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë„ ë‹¤ë¦„ (8k ~ 128k)
- ì»¨í…ìŠ¤íŠ¸ ì¦ê°€ ì‹œ VRAM ì‚¬ìš©ëŸ‰ë„ ì¦ê°€

**í•´ê²°**:
```typescript
// 1. ì‚¬ìš©ì ì„¤ì • ê°€ëŠ¥í•˜ê²Œ
interface AiConfigState {
  contextLimit?: number;  // í† í° ë‹¨ìœ„ (Ollama num_ctxì— í•´ë‹¹)
}

// 2. ë³´ìˆ˜ì  ê¸°ë³¸ê°’ (Ollama ê¸°ë³¸ 2048 ê³ ë ¤)
function getDefaultContextLimit(baseUrl?: string): number {
  const isLocal = baseUrl?.match(/localhost|127\.0\.0\.1|0\.0\.0\.0/);
  // ë¡œì»¬: 8k (2048ë³´ë‹¤ ë†’ì§€ë§Œ ëŒ€ë¶€ë¶„ ëª¨ë¸ì—ì„œ ì•ˆì „)
  // ë‹¨, ì‚¬ìš©ìì—ê²Œ Ollama ì„¤ì • í•„ìš”í•¨ì„ ì•ˆë‚´
  return isLocal ? 8_000 : (provider === 'anthropic' ? 200_000 : 400_000);
}

// 3. ì‚¬ì „ ê²€ì¦ (OllamaëŠ” ì´ˆê³¼í•´ë„ ì—ëŸ¬ ì—†ì´ ì˜ë¦¼!)
if (estimatedTokens > contextLimit * 0.9) {
  throw new Error(`ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì´ˆê³¼: ${estimatedTokens} > ${contextLimit}`);
}

// 4. ìë™ ì²­í‚¹ (ê¸°ì¡´ ë¡œì§ í™œìš©)
if (estimatedTokens > contextLimit * 0.6) {
  return translateSourceDocWithChunking(params);
}
```

**âš ï¸ Ollama ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ ì‹œ ë™ì‘**:
- **ì—ëŸ¬ ì—†ì´ ì¡°ìš©íˆ ì˜ë¦¼ (truncation)** - ë§¤ìš° ìœ„í—˜!
- ë²ˆì—­ ì‹œ ë¬¸ì„œ ë’·ë¶€ë¶„ ëˆ„ë½
- ì±„íŒ… ì‹œ ì´ì „ ëŒ€í™” ë§¥ë½ ì†ì‹¤
- **â†’ ì•±ì—ì„œ ì‚¬ì „ ê²€ì¦ í•„ìˆ˜**

**ì‚¬ìš©ì ì•ˆë‚´ í•„ìš”**:
```
âš ï¸ Ollama ì‚¬ìš© ì‹œ ì»¨í…ìŠ¤íŠ¸ ì„¤ì • í•„ìš”
Ollama ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ëŠ” 2048 í† í°ì…ë‹ˆë‹¤.
ë” ê¸´ ë¬¸ì„œë¥¼ ë²ˆì—­í•˜ë ¤ë©´ Ollama ì„¤ì •ì„ ë³€ê²½í•˜ì„¸ìš”:
  OLLAMA_CONTEXT_LENGTH=32768 ollama serve
```

### 3. Responses API

**ë¬¸ì œ**: `useResponsesApi: true`ëŠ” OpenAI ì „ìš©

**í•´ê²°**:
```typescript
// client.ts
const useResponsesApi =
  useFor === 'chat' &&
  !isLocalEndpoint(cfg.openaiBaseUrl);  // ë¡œì»¬ì´ë©´ ë¹„í™œì„±í™”

return new ChatOpenAI({
  apiKey: cfg.openaiApiKey,
  model,
  configuration: cfg.openaiBaseUrl ? { baseURL: cfg.openaiBaseUrl } : undefined,
  ...(useResponsesApi ? { useResponsesApi: true } : {}),
});
```

### 4. ë‚´ì¥ ì›¹ ê²€ìƒ‰

**ë¬¸ì œ**: `web_search_preview` (OpenAI), `web_search` (Anthropic)ëŠ” ê³µì‹ API ì „ìš©

**í•´ê²°**:
```typescript
// chat.ts
const builtInWebSearchTools =
  webSearchEnabled && !isLocalEndpoint(cfg.openaiBaseUrl)
    ? getBuiltInWebSearchTool(cfg.provider)
    : [];
```

**UX ì˜í–¥**: ë¡œì»¬ LLM ì‚¬ìš© ì‹œ ë‚´ì¥ ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™” (í† ê¸€ ìˆ¨ê¹€ ë˜ëŠ” ë¹„í™œì„±í™” í‘œì‹œ)

### 5. Vision (ì´ë¯¸ì§€ ì…ë ¥)

**ë¬¸ì œ**: ì¼ë¶€ ëª¨ë¸ë§Œ ì§€ì› (llava, llama3.2-vision)

**í•´ê²°**: ê¸°ì¡´ í´ë°± ë¡œì§ í™œìš© (ì´ë¯¸ êµ¬í˜„ë¨)
```typescript
// chat.ts:702-722 - ì´ë¯¸ì§€ ì…ë ¥ ì‹¤íŒ¨ ì‹œ ì´ë¯¸ì§€ ì œì™¸í•˜ê³  ì¬ì‹œë„
if (usedImages) {
  const fallback = replaceLastHumanMessageText(
    messagesWithGuide,
    `${input.userMessage}\n\n[ì´ë¯¸ì§€ ì…ë ¥ì´ ì§€ì›ë˜ì§€ ì•Šì•„ ì œì™¸ë¨]`,
  );
  ({ finalText } = await runToolCallingLoop({ model, tools, bindTools, messages: fallback }));
}
```

### 6. max_tokens ì¶œë ¥ ì œí•œ

**ë¬¸ì œ**: ëª¨ë¸ë³„ ì¶œë ¥ í† í° ì œí•œ ë‹¤ë¦„

**í•´ê²°**:
```typescript
// ì‚¬ìš©ì ì„¤ì • ë˜ëŠ” ë³´ìˆ˜ì  ê¸°ë³¸ê°’
const maxOutputTokens = cfg.maxOutputTokens ?? (isLocalEndpoint ? 4096 : 65536);
```

---

## êµ¬í˜„ ê³„íš

### Phase 1: ì„¤ì • ì¸í”„ë¼ (í•„ìˆ˜)

#### 1.1 aiConfigStore í™•ì¥
```typescript
// src/stores/aiConfigStore.ts
interface AiConfigState {
  // ê¸°ì¡´ í•„ë“œ...

  // ì‹ ê·œ í•„ë“œ
  openaiBaseUrl?: string;       // ì»¤ìŠ¤í…€ ì—”ë“œí¬ì¸íŠ¸ (ì˜ˆ: http://localhost:11434/v1)
  contextLimit?: number;        // ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (í† í°), ê¸°ë³¸ê°’: ìë™
  maxOutputTokens?: number;     // ì¶œë ¥ í† í° ì œí•œ, ê¸°ë³¸ê°’: 4096 (ë¡œì»¬)
  customModelName?: string;     // ì»¤ìŠ¤í…€ ëª¨ë¸ëª… (í”„ë¦¬ì…‹ ì™¸)
}
```

#### 1.2 config.ts ìˆ˜ì •
```typescript
// src/ai/config.ts
export function getAiConfig(options?: AiConfigOptions): AiConfig {
  const store = useAiConfigStore.getState();

  return {
    // ê¸°ì¡´...
    openaiBaseUrl: store.openaiBaseUrl,
    contextLimit: store.contextLimit ?? getDefaultContextLimit(store.openaiBaseUrl),
    maxOutputTokens: store.maxOutputTokens,
  };
}

function getDefaultContextLimit(baseUrl?: string): number {
  if (isLocalEndpoint(baseUrl)) return 16_000;
  return 400_000;  // OpenAI ê¸°ë³¸
}

export function isLocalEndpoint(baseUrl?: string): boolean {
  if (!baseUrl) return false;
  return /localhost|127\.0\.0\.1|0\.0\.0\.0|192\.168\.|10\.\d+\./.test(baseUrl);
}
```

#### 1.3 client.ts ìˆ˜ì •
```typescript
// src/ai/client.ts
export function createChatModel(modelOverride?: string, options?: ModelOptions): BaseChatModel {
  const cfg = getAiConfig(options);
  const model = modelOverride ?? cfg.model;
  const isLocal = isLocalEndpoint(cfg.openaiBaseUrl);

  if (cfg.provider === 'openai' || cfg.provider === 'mock') {
    // API í‚¤: ë¡œì»¬ì´ë©´ ë”ë¯¸ ê°’ í—ˆìš©
    const apiKey = isLocal ? (cfg.openaiApiKey || 'ollama') : cfg.openaiApiKey;
    if (!apiKey && !isLocal) throw new Error(i18n.t('errors.openaiApiKeyMissing'));

    const useResponsesApi = !isLocal && useFor === 'chat';

    return new ChatOpenAI({
      apiKey: apiKey || 'ollama',
      model,
      configuration: cfg.openaiBaseUrl ? { baseURL: cfg.openaiBaseUrl } : undefined,
      ...temperatureOption,
      ...maxTokensOption,
      ...(useResponsesApi ? { useResponsesApi: true } : {}),
    });
  }
  // Anthropicì€ ê¸°ì¡´ ë¡œì§ ìœ ì§€
}
```

#### 1.4 ì—°ê²° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ì‹ ê·œ)
```typescript
// src/ai/ollamaUtils.ts

/**
 * Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
 * Settings UIì—ì„œ "Test Connection" ë²„íŠ¼ìš©
 */
export async function testOllamaConnection(baseUrl: string): Promise<{
  success: boolean;
  models?: string[];
  error?: string;
}> {
  try {
    // OllamaëŠ” /api/tagsë¡œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ê°€ëŠ¥
    const apiBase = baseUrl.replace('/v1', '');
    const res = await fetch(`${apiBase}/api/tags`);
    if (!res.ok) throw new Error(`HTTP ${res.status}`);

    const data = await res.json();
    const models = data.models?.map((m: any) => m.name) ?? [];

    return { success: true, models };
  } catch (e) {
    return {
      success: false,
      error: e instanceof Error ? e.message : 'Connection failed',
    };
  }
}

/**
 * íŠ¹ì • ëª¨ë¸ì˜ Tool Calling ì§€ì› ì—¬ë¶€ í™•ì¸
 * (Ollama APIë¡œ ëª¨ë¸ ì •ë³´ ì¡°íšŒ)
 */
export async function checkModelCapabilities(baseUrl: string, modelName: string): Promise<{
  supportsTools: boolean;
  contextLength?: number;
}> {
  try {
    const apiBase = baseUrl.replace('/v1', '');
    const res = await fetch(`${apiBase}/api/show`, {
      method: 'POST',
      body: JSON.stringify({ name: modelName }),
    });
    if (!res.ok) return { supportsTools: false };

    const data = await res.json();
    // Ollama ì‘ë‹µì—ì„œ templateì— "tools" í¬í•¨ ì—¬ë¶€ë¡œ íŒë‹¨ (íœ´ë¦¬ìŠ¤í‹±)
    const template = data.template ?? '';
    const supportsTools = template.includes('tool') || template.includes('function');

    // ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ëŠ” modelfileì—ì„œ ì¶”ì¶œ
    const contextMatch = data.parameters?.match(/num_ctx\s+(\d+)/);
    const contextLength = contextMatch ? parseInt(contextMatch[1]) : undefined;

    return { supportsTools, contextLength };
  } catch {
    return { supportsTools: false };
  }
}
```

### Phase 2: ë²ˆì—­ ê¸°ëŠ¥ (í•„ìˆ˜)

#### 2.1 translateDocument.ts ìˆ˜ì •
```typescript
// ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°
const MAX_CONTEXT = cfg.contextLimit ??
  (cfg.provider === 'anthropic' ? 200_000 : 400_000);

// max_tokensë„ ì„¤ì •ì—ì„œ
const maxAllowedTokens = cfg.maxOutputTokens ??
  (cfg.provider === 'anthropic' ? 64000 :
   isLocalEndpoint(cfg.openaiBaseUrl) ? 4096 :
   cfg.model?.startsWith('gpt-5') ? 65536 : 16384);
```

#### 2.2 ì²­í‚¹ ì„ê³„ê°’ ë™ì  ì¡°ì •
```typescript
// src/ai/chunking.ts
const CHUNK_THRESHOLD_RATIO = 0.6;  // ì»¨í…ìŠ¤íŠ¸ì˜ 60%

function shouldUseChunking(tokens: number, contextLimit: number): boolean {
  return tokens > contextLimit * CHUNK_THRESHOLD_RATIO;
}
```

### Phase 3: ì±„íŒ… ê¸°ëŠ¥ (í•„ìˆ˜)

#### 3.1 Tool Calling í´ë°±
```typescript
// src/ai/chat.ts

async function runWithToolCallingFallback(params: {
  model: BaseChatModel;
  tools: Tool[];
  messages: BaseMessage[];
  sourceDoc?: string;
  targetDoc?: string;
}): Promise<{ finalText: string; toolsUsed: string[] }> {
  try {
    return await runToolCallingLoop({
      model: params.model,
      tools: params.tools,
      messages: params.messages,
    });
  } catch (e) {
    if (isToolCallingNotSupported(e)) {
      console.warn('[AI] Tool calling not supported, falling back to simple chat');

      // ë¬¸ì„œë¥¼ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì§ì ‘ í¬í•¨
      const enrichedMessages = injectDocumentsToSystemMessage(
        params.messages,
        params.sourceDoc,
        params.targetDoc,
      );

      // ë‹¨ìˆœ invoke (tool calling ì—†ì´)
      const result = await params.model.invoke(enrichedMessages);
      return {
        finalText: extractTextContent(result),
        toolsUsed: [],
      };
    }
    throw e;
  }
}

function isToolCallingNotSupported(error: unknown): boolean {
  if (!(error instanceof Error)) return false;
  const msg = error.message.toLowerCase();
  return (
    msg.includes('tool') && (
      msg.includes('not supported') ||
      msg.includes('unsupported') ||
      msg.includes('invalid')
    )
  );
}
```

#### 3.2 ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”
```typescript
// chat.ts
const builtInWebSearchTools =
  webSearchEnabled && !isLocalEndpoint(cfg.openaiBaseUrl)
    ? getBuiltInWebSearchTool(cfg.provider)
    : [];
```

### Phase 4: Settings UI (í•„ìˆ˜)

#### 4.1 Local LLM ì„¤ì • ì„¹ì…˜ ì¶”ê°€
```tsx
// src/components/settings/AppSettingsModal.tsx

{/* Local LLM Settings */}
<SettingsSection title={t('settings.localLlm.title')}>
  {/* ì—”ë“œí¬ì¸íŠ¸ ì…ë ¥ */}
  <SettingsRow label={t('settings.localLlm.endpoint')}>
    <div className="flex gap-2">
      <input
        type="text"
        placeholder="http://localhost:11434/v1"
        value={openaiBaseUrl}
        onChange={(e) => setOpenaiBaseUrl(e.target.value)}
        className="flex-1"
      />
      <Button
        onClick={handleTestConnection}
        loading={testing}
      >
        {t('settings.localLlm.testConnection')}
      </Button>
    </div>
    <HelperText>
      Ollama: http://localhost:11434/v1 | LM Studio: http://localhost:1234/v1
    </HelperText>
  </SettingsRow>

  {/* ì—°ê²° ì„±ê³µ ì‹œ ëª¨ë¸ ëª©ë¡ í‘œì‹œ */}
  {availableModels.length > 0 && (
    <SettingsRow label={t('settings.localLlm.model')}>
      <Select
        value={customModelName}
        onChange={setCustomModelName}
        options={availableModels.map(m => ({ value: m, label: m }))}
      />
    </SettingsRow>
  )}

  {/* ì»¨í…ìŠ¤íŠ¸ ì œí•œ */}
  <SettingsRow label={t('settings.localLlm.contextLimit')}>
    <input
      type="number"
      placeholder="8000"
      value={contextLimit}
      onChange={(e) => setContextLimit(Number(e.target.value))}
    />
    <HelperText>
      âš ï¸ Ollama ê¸°ë³¸ê°’ì€ 2048ì…ë‹ˆë‹¤. ì„œë²„ì—ì„œ OLLAMA_CONTEXT_LENGTHë¥¼ ì„¤ì •í•˜ì„¸ìš”.
    </HelperText>
  </SettingsRow>

  {/* ìµœëŒ€ ì¶œë ¥ í† í° */}
  <SettingsRow label={t('settings.localLlm.maxOutput')}>
    <input
      type="number"
      placeholder="4096"
      value={maxOutputTokens}
      onChange={(e) => setMaxOutputTokens(Number(e.target.value))}
    />
  </SettingsRow>

  {/* Ollama ì„¤ì • ì•ˆë‚´ */}
  <Callout type="info">
    <strong>ğŸ’¡ Ollama ì„¤ì • íŒ</strong>
    <ul>
      <li>ë” ê¸´ ë¬¸ì„œ ë²ˆì—­: <code>OLLAMA_CONTEXT_LENGTH=32768 ollama serve</code></li>
      <li>Tool Calling ì§€ì› ëª¨ë¸: llama3.1, qwen2.5, mistral</li>
      <li>ì¶”ì²œ ëª¨ë¸: <code>ollama pull llama3.1:8b</code></li>
    </ul>
  </Callout>
</SettingsSection>
```

#### 4.2 ì—°ê²° í…ŒìŠ¤íŠ¸ í•¸ë“¤ëŸ¬
```typescript
const [testing, setTesting] = useState(false);
const [availableModels, setAvailableModels] = useState<string[]>([]);

const handleTestConnection = async () => {
  if (!openaiBaseUrl) return;

  setTesting(true);
  try {
    const result = await testOllamaConnection(openaiBaseUrl);
    if (result.success) {
      setAvailableModels(result.models ?? []);
      toast.success(t('settings.localLlm.connectionSuccess'));
    } else {
      toast.error(t('settings.localLlm.connectionFailed', { error: result.error }));
    }
  } finally {
    setTesting(false);
  }
};
```

#### 4.3 ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´ í™•ì¥
```typescript
// ì»¤ìŠ¤í…€ ëª¨ë¸ëª… ì…ë ¥ í—ˆìš©
const modelOptions = [
  ...presetModels,
  // Ollama ëª¨ë¸ ëª©ë¡ (ì—°ê²° í…ŒìŠ¤íŠ¸ í›„)
  ...(availableModels.length > 0 ? [{
    label: 'Ollama Models',
    options: availableModels.map(m => ({ value: m, label: m })),
  }] : []),
  // ì§ì ‘ ì…ë ¥í•œ ì»¤ìŠ¤í…€ ëª¨ë¸
  ...(customModelName && !availableModels.includes(customModelName)
    ? [{ value: customModelName, label: `Custom: ${customModelName}` }]
    : []),
];
```

### Phase 5: i18n (í•„ìˆ˜)

```json
// src/i18n/locales/ko.json
{
  "settings": {
    "localLlm": {
      "title": "ë¡œì»¬ LLM ì„¤ì •",
      "endpoint": "API ì—”ë“œí¬ì¸íŠ¸",
      "endpointHelp": "Ollama, LM Studio ë“±ì˜ OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸",
      "contextLimit": "ì»¨í…ìŠ¤íŠ¸ ì œí•œ (í† í°)",
      "contextLimitHelp": "ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°. ëª¨ë¥´ë©´ 16000 ê¶Œì¥.",
      "maxOutput": "ìµœëŒ€ ì¶œë ¥ í† í°",
      "customModel": "ì»¤ìŠ¤í…€ ëª¨ë¸ëª…"
    }
  },
  "errors": {
    "contextLimitExceeded": "ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì´ˆê³¼: {{actual}} > {{limit}} í† í°",
    "toolCallingNotSupported": "ì´ ëª¨ë¸ì€ ë„êµ¬ í˜¸ì¶œì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤."
  }
}
```

---

## ê¸°ëŠ¥ë³„ ì˜í–¥ë„

| ê¸°ëŠ¥ | ë¡œì»¬ LLM ì§€ì› | ì œí•œì‚¬í•­ |
|------|-------------|----------|
| **ë²ˆì—­** | âœ… ì™„ì „ ì§€ì› | ê¸´ ë¬¸ì„œëŠ” ìë™ ì²­í‚¹ |
| **ì±„íŒ… (ê¸°ë³¸)** | âœ… ì§€ì› | - |
| **ì±„íŒ… (ë¬¸ì„œ ì¡°íšŒ)** | âš ï¸ ì¡°ê±´ë¶€ | Tool Calling ë¯¸ì§€ì› ì‹œ ë¬¸ì„œ ì§ì ‘ í¬í•¨ |
| **ì±„íŒ… (ì›¹ ê²€ìƒ‰)** | âŒ ë¯¸ì§€ì› | ê³µì‹ API ì „ìš© ê¸°ëŠ¥ |
| **ì±„íŒ… (ì´ë¯¸ì§€)** | âš ï¸ ì¡°ê±´ë¶€ | Vision ëª¨ë¸ë§Œ ì§€ì› |
| **ë²ˆì—­ ê²€ìˆ˜** | âœ… ì§€ì› | ì²­í‚¹ìœ¼ë¡œ ì²˜ë¦¬ |
| **ê·œì¹™/ì»¨í…ìŠ¤íŠ¸ ì œì•ˆ** | âš ï¸ ì¡°ê±´ë¶€ | Tool Calling ë¯¸ì§€ì› ì‹œ ë¶ˆê°€ |

---

## í…ŒìŠ¤íŠ¸ ê³„íš

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- [ ] `isLocalEndpoint()` í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
- [ ] `getDefaultContextLimit()` í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
- [ ] `testOllamaConnection()` í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
- [ ] `checkModelCapabilities()` í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
- [ ] Tool Calling í´ë°± ë¡œì§ í…ŒìŠ¤íŠ¸
- [ ] ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ ê°ì§€ í…ŒìŠ¤íŠ¸

### í†µí•© í…ŒìŠ¤íŠ¸
- [ ] Ollama + llama3.1 ë²ˆì—­ í…ŒìŠ¤íŠ¸ (Tool Calling ì§€ì›)
- [ ] Ollama + qwen2.5 ì±„íŒ… í…ŒìŠ¤íŠ¸ (Tool Calling ì§€ì›)
- [ ] Ollama + phi-3 ì±„íŒ… í…ŒìŠ¤íŠ¸ (Tool Calling ë¯¸ì§€ì› â†’ í´ë°±)
- [ ] LM Studio ì—°ë™ í…ŒìŠ¤íŠ¸
- [ ] ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ ì‹œ ì²­í‚¹ ë™ì‘ í…ŒìŠ¤íŠ¸

### ìˆ˜ë™ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ë³¸ ì„¤ì •
```bash
# 1. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.1:8b

# 2. ì»¨í…ìŠ¤íŠ¸ í™•ì¥í•˜ì—¬ ì„œë²„ ì‹œì‘
OLLAMA_CONTEXT_LENGTH=32768 ollama serve
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ì•±ì—ì„œ í…ŒìŠ¤íŠ¸
1. Settings â†’ Local LLM â†’ ì—”ë“œí¬ì¸íŠ¸ ì…ë ¥: `http://localhost:11434/v1`
2. "Test Connection" í´ë¦­ â†’ ëª¨ë¸ ëª©ë¡ í‘œì‹œ í™•ì¸
3. ëª¨ë¸ ì„ íƒ: `llama3.1:8b`
4. ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì„¤ì •: `16000`

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ë²ˆì—­ í…ŒìŠ¤íŠ¸
1. ì§§ì€ ë¬¸ì„œ (1000ì ì´í•˜) â†’ ë‹¨ì¼ í˜¸ì¶œ ì„±ê³µ
2. ì¤‘ê°„ ë¬¸ì„œ (5000ì) â†’ ë‹¨ì¼ í˜¸ì¶œ ì„±ê³µ
3. ê¸´ ë¬¸ì„œ (20000ì) â†’ ì²­í‚¹ ë™ì‘ í™•ì¸ (ì§„í–‰ë¥  í‘œì‹œ)

#### ì‹œë‚˜ë¦¬ì˜¤ 4: ì±„íŒ… í…ŒìŠ¤íŠ¸
1. ì¼ë°˜ ì§ˆë¬¸ â†’ ì‘ë‹µ í™•ì¸
2. "ì›ë¬¸ì„ ìš”ì•½í•´ì¤˜" â†’ Tool Calling (ë¬¸ì„œ ì¡°íšŒ) ë™ì‘ í™•ì¸
3. Tool Calling ë¯¸ì§€ì› ëª¨ë¸ â†’ í´ë°± ë™ì‘ í™•ì¸

#### ì‹œë‚˜ë¦¬ì˜¤ 5: ì—ëŸ¬ ì¼€ì´ìŠ¤
1. ì„œë²„ ë¯¸ì‹¤í–‰ â†’ ì—°ê²° ì‹¤íŒ¨ ì—ëŸ¬ í‘œì‹œ
2. ì˜ëª»ëœ ëª¨ë¸ëª… â†’ ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ
3. ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ â†’ ì²­í‚¹ìœ¼ë¡œ ìë™ ì „í™˜

---

## ë§ˆì´ê·¸ë ˆì´ì…˜

### ê¸°ì¡´ ì‚¬ìš©ì ì˜í–¥
- ì—†ìŒ (ì‹ ê·œ ì„¤ì • í•„ë“œëŠ” ì˜µì…˜)
- `openaiBaseUrl` ë¯¸ì„¤ì • ì‹œ ê¸°ì¡´ ë™ì‘ ìœ ì§€

### ì„¤ì • ë§ˆì´ê·¸ë ˆì´ì…˜
```typescript
// aiConfigStore.ts - persist ë²„ì „ ì—…ë°ì´íŠ¸
const STORE_VERSION = 2;  // 1 â†’ 2

migrate: (persisted, version) => {
  if (version < 2) {
    return {
      ...persisted,
      openaiBaseUrl: undefined,
      contextLimit: undefined,
      maxOutputTokens: undefined,
      customModelName: undefined,
    };
  }
  return persisted;
}
```

---

## ì˜ˆìƒ ì‘ì—…ëŸ‰

| Phase | ì‘ì—… | ì˜ˆìƒ ê·œëª¨ |
|-------|------|----------|
| 1 | ì„¤ì • ì¸í”„ë¼ | ì¤‘ |
| 2 | ë²ˆì—­ ê¸°ëŠ¥ | ì†Œ |
| 3 | ì±„íŒ… ê¸°ëŠ¥ (í´ë°±) | ì¤‘ |
| 4 | Settings UI | ì¤‘ |
| 5 | i18n | ì†Œ |
| - | í…ŒìŠ¤íŠ¸ | ì¤‘ |

---

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Ollama OpenAI Compatibility](https://ollama.com/blog/openai-compatibility)
- [Ollama Tool Calling](https://docs.ollama.com/capabilities/tool-calling)
- [Ollama Model Library](https://ollama.com/library)
- [LangChain ChatOpenAI](https://js.langchain.com/docs/integrations/chat/openai)
- [LangChain ChatOllama](https://js.langchain.com/docs/integrations/chat/ollama/)

### ê´€ë ¨ ê°€ì´ë“œ
- [Best Ollama Models for Function Calling 2025](https://collabnix.com/best-ollama-models-for-function-calling-tools-complete-guide-2025/)
- [How to Increase Context Length in Ollama](https://localllm.in/blog/local-llm-increase-context-length-ollama)
- [Ollama Context Window](https://blog.driftingruby.com/ollama-context-window/)

### LM Studio / ëŒ€ì•ˆ
- [LM Studio](https://lmstudio.ai/) - GUI ê¸°ë°˜ ë¡œì»¬ LLM (OpenAI í˜¸í™˜ API ì œê³µ)
- [vLLM](https://github.com/vllm-project/vllm) - ê³ ì„±ëŠ¥ LLM ì„œë¹™
